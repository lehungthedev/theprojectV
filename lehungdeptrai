import os
while True:
    try:
        import requests
        import os
        from Naked.toolshed.shell import execute_js, muterun_js
        import sys
        import base64
        import threading
        import time
        break
    except:
        os.system('pip install naked requests')
        continue
os.system('rm proxy.txt')
class mining:
    def __init__(self): 
        open('hidemycookie.txt','a')
        open('math1.js','a')
        open('math.js','a')
        self.prxcnt = 0
        self.client = requests.Session()
        self.col = {'red':'\033[91m','green':'\033[92m','yellow':'\033[93;1m','blue':'\033[94m','magenta':'\033[95m','cyan':'\033[96m','white':'\033[1;37m'}
        self.banner()
        self.hmcookie = ''
        if open('hidemycookie.txt','r').read() == '':
        	self.hmcookie = input('enter hidemy cookie[enter for skip]:'+self.col['white'])
        	open('hidemycookie.txt','a').write(self.hmcookie)
        else:
        	self.hmcookie = open('hidemycookie.txt','r').read()
        self.banner()
    def encode(self,string):
        sample_string = string
        sample_string_bytes = sample_string.encode("utf-8") 
        base64_bytes = base64.b64encode(sample_string_bytes) 
        base64_string = base64_bytes.decode("utf-8") 
        return base64_string
    def decode(self,string):
        base64_string = string
        base64_bytes = base64_string.encode("utf-8") 
        sample_string_bytes = base64.b64decode(base64_bytes) 
        sample_string = sample_string_bytes.decode("utf-8") 
        return sample_string
    def status(self,data):
        self.prxcnt += len(data)
        print(f'''{self.col['magenta']}mining {self.prxcnt} proxies{self.col['white']}''',end = '\r')
    def total(self):
        print(self.col['green']+'vui long cho, dang xu li du lieu...',end = '\r')
        open('proxy.txt','a')
        data = open('proxy.txt','r').read().split('\n')
        loc = []
        for i in data:
            if i not in loc:
                loc.append(i)
        open('proxy.txt','w').write('\n'.join(loc))
        print('luu proxy vao file proxy.txt             ')
        print('dao thanh cong '+self.col['green']+str(len(open('proxy.txt','r').read().split('\n')))+self.col['white']+' proxies , loai bo '+str(self.prxcnt-len(open('proxy.txt','r').read().split('\n')))+' proxy giong nhau               ')
        os.system('rm math.js math1.js')
    def banner(self):
        os.system('clear')
        orange = ["\033[38;5;202m","\033[38;5;208m","\033[38;5;214m","\033[38;5;220m","\033[38;5;226m","\033[38;5;190m"]
        #┘┌└┐
        bn = f'''
                                                         
            {orange[0]}██╗  ██╗███╗   ██╗ ██████╗ ██╗     
            {orange[1]}██║  ██║████╗  ██║██╔════╝ ██║     
            {orange[2]}███████║██╔██╗ ██║██║  ███╗██║     
            {orange[3]}██╔══██║██║╚██╗██║██║   ██║██║     
            {orange[4]}██║  ██║██║ ╚████║╚██████╔╝███████╗
            {orange[5]}╚═╝  ╚═╝╚═╝  ╚═══╝ ╚═════╝ ╚══════╝


   {self.col['green']}╚>{self.col['blue']}Facebook:{self.col['white']}https://www.facebook.com/100054423332132
   {self.col['green']}╚>{self.col['blue']}Zalo:{self.col['white']}https://zaloapp.com/qr/p/13zymfkf7s3k1/
   {self.col['green']}╚>{self.col['red']}Youtube:{self.col['white']}https://www.youtube.com/@rtech2808/            
            {self.col['cyan']}TOOL DAO PROXY BAN QUYEN HNGL
       {self.col['cyan']}Vui long khong tat tool den khi hoan tat
                                                {self.col['magenta']}@Lehung{self.col['red']}'''
        print(bn)
    def free_proxy_list(self):
        def thread1():
            data = requests.get('https://free-proxy-list.net').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        def thread2():
            data = requests.get('https://www.socks-proxy.net/').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        def thread3():
            data = requests.get('https://www.us-proxy.org/').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        def thread4():
            data = requests.get('https://free-proxy-list.net/uk-proxy.html').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        def thread5():
            data = requests.get('https://www.sslproxies.org/').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        def thread6():
            data = requests.get('https://free-proxy-list.net/anonymous-proxy.html').text
            proxy = data.split('UTC.')[1].split('<')[0].split('\n')
            proxy.pop(0)
            proxy.pop(1)
            proxy.pop(-1)
            print(f'''[{self.col['red']}+{self.col['white']}] freeproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        th1 = threading.Thread(target=thread1)
        th2 = threading.Thread(target=thread2)
        th3 = threading.Thread(target=thread3)
        th4 = threading.Thread(target=thread4)
        th5 = threading.Thread(target=thread5)
        th6 = threading.Thread(target=thread6)
        th1.start()
        th2.start()
        th3.start()
        th4.start()
        th5.start()
        th6.start()
        th1.join()
        th2.join()
        th3.join()
        th4.join()
        th5.join()
        th6.join()
    def vpnoverview(self):
        header = {
            'method':'GET',
            'path':'/privacy/anonymous-browsing/free-proxy-servers/',
            'scheme':'https',
            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language':'en-US,en;q=0.9,vi;q=0.8',
            'Cache-Control':'max-age=0',
            'Cookie':'eaf_url=https//vpnoverview.com/privacy/anonymous-browsing/free-proxy-servers/; eaf_vid=654cf08b68e267.98275717',
            'Referer':'https//duckduckgo.com/',
            'Sec-Ch-Ua':'"Chromium";v="119", "Not?A_Brand";v="24"',
            'Sec-Ch-Ua-Mobile':'?0',
            'Sec-Ch-Ua-Platform':'"Linux"',
            'Sec-Fetch-Dest':'document',
            'Sec-Fetch-Mode':'navigate',
            'Sec-Fetch-Site':'cross-site',
            'Sec-Fetch-User':'?1',
            'Upgrade-Insecure-Requests':'1',
            'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',

        }
        data = requests.get('https://vpnoverview.com/privacy/anonymous-browsing/free-proxy-servers/',headers=header).text
        proxy = []
        cnt,i = data.count('<td><strong>'),0
        while '<td><strong>' in data:
            proxy.append(data.split('<td><strong>')[1].split('<')[0])
            proxy[i] += ':'+data[data.index(proxy[i]):].split('</td><td>')[1].split('</td>')[0]
            data = data.replace('<td><strong>','encrypthnglt',1)
            print(f'''[{self.col['red']}+{self.col['white']}] vpnoverview {self.col['red']}•{self.col['white']} {i+1} proxies added {self.col['red']}•{self.col['white']} {int((i/cnt)*100)}%''',end = '\r')
            i+=1
        print(f'''[{self.col['red']}+{self.col['white']}] vpnoverview {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        open('proxy.txt','a').write('\n'.join(proxy))
        self.status(proxy)
        return open('proxy.txt','a').write('\n'.join(proxy))
    def hidemy_io(self):
        def extractdata(data,page):
            extract = []
            data = data.replace('<tr><td>','encrypthngl',1)
            while '<tr><td>' in data:
                extract.append(data.split('<tr><td>')[1].split('<')[0])
                port = data[data.index(extract[-1]):].split('</td><td>')[1].split('<')[0]
                extract[-1] += ':'+port
                data = data.replace('<tr><td>','encrypthngl',1).replace(f'</td><td>{port}<','encrypthngl',1)
            print(f'''[{self.col['red']}+{self.col['white']}] hidemy {self.col['red']}•{self.col['white']} {len(extract)} proxies added {self.col['red']}•{self.col['white']} page {page} {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            open('proxy.txt','a').write('\n'.join(extract))
            self.status(extract)
            return open('proxy.txt','a').write('\n')
        try:
            header = {
                'authority':'hidemy.io',
                'method':'GET',
                'scheme':'https',
                'Cookie':self.hmcookie,
                'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language':'en-US,en;q=0.9,vi;q=0.8',
                'Sec-Ch-Ua':'"Chromium";v="119", "Not?A_Brand";v="24"',
                'Sec-Ch-Ua-Mobile':'?0',
                'Sec-Ch-Ua-Platform':'"Linux"',
                'Sec-Fetch-Dest':'document',
                'Sec-Fetch-Mode':'navigate',
                'Sec-Fetch-Site':'none',
                'Sec-Fetch-User':'?1',
                'Upgrade-Insecure-Requests':'1',
                'User-Agent': (self.hmcookie.replace('%20',' ').replace('%3B',';').replace('%2F','/').replace('%2C',',').split('_uafec=')[1])[:-2]}
        except:
            header = {
                'authority':'hidemy.io',
                'method':'GET',
                'scheme':'https',
                'Cookie':self.hmcookie,
                'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language':'en-US,en;q=0.9,vi;q=0.8',
                'Sec-Ch-Ua':'"Chromium";v="119", "Not?A_Brand";v="24"',
                'Sec-Ch-Ua-Mobile':'?0',
                'Sec-Ch-Ua-Platform':'"Linux"',
                'Sec-Fetch-Dest':'document',
                'Sec-Fetch-Mode':'navigate',
                'Sec-Fetch-Site':'none',
                'Sec-Fetch-User':'?1',
                'Upgrade-Insecure-Requests':'1',
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
        data = self.client.get('https://hidemy.io/en/proxy-list/',headers=header).text
        extractdata(data,0)
        page = []
        for i in range(data.count('#list">')):
            page.append(data.split('#list">')[1].split('<')[0])
            data = data.replace('#list">','encrypthnglt',1)
            if page[-1] == '':
                page.pop(-1)
        page = max(list(map(int,page)))
        for i in range(1,page):
            url = f'https://hidemy.io/en/proxy-list/?start={i*64}#list'
            data = self.client.get(url,headers=header).text
            if 'Just a moment...' in data:
                print(self.col['red']+'cloudflare block'+self.col['white'],end = '\r')
                os.system('rm hidemycookie.txt')
                break
            extractdata(data,i)
    def proxynova(self):
        def extract(data,region):
            os.system('rm math.js')
            proxy,porta = [],[]
            cnt = data.count('<script>document.write(')
            open('math.js','a').write('function greet() {\n')
            for i in range(cnt):
                ip = data.split('<script>document.write(')[1].split(')</script>')[0]
                port = data[data.index(ip):].split('<td align="left">')[1].split('</td>')[0]
                while 'atob' in ip:
                    dec = ip.split('atob("')[1].split('"')[0]
                    ip = ip.replace(f'atob("{dec}")',f'"{self.decode(dec)}"')
                open('math.js','a').write('      console.log('+ip+');\n')
                open('math.js','a').write('      console.log(" ");\n')
                data = data.replace('<script>document.write(','encrypt',1).replace('</script>','encrypt',1)
                if 'title="Port ' in port:
                    port = port.split('title="Port ')[1].split(' ')[0]
                else:
                    port = port.replace('\n','').replace(' ','')
                if ip != '':
                    print(f'''[{self.col['red']}+{self.col['white']}] proxynova {self.col['red']}•{self.col['white']} {i} proxies added {self.col['red']}•{self.col['white']} {region} {self.col['red']}•{self.col['white']} {int((i/cnt)*100)}%''',end = '\r')
                    porta.append(port)
            open('math.js','a').write('''            }\ngreet()''')
            ip = str(muterun_js('math.js').stdout).replace('b','').replace('\\n','').replace("'",'').split(' ')
            for i in range(len(ip)-1):
                proxy.append(ip[i]+':'+porta[i])
            print(f'''[{self.col['red']}+{self.col['white']}] proxynova {self.col['red']}•{self.col['white']} {i} proxies added {self.col['red']}•{self.col['white']} {region} {self.col['red']}•{self.col['white']} 100%''',end = '\r')
            print()
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            self.status(proxy)
        header = {
            'authority':'www.proxynova.com',
            'method':'GET',
            'scheme':'https',
            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language':'en-US,en;q=0.9,vi;q=0.8',
            'Cache-Control':'max-age=0',
            'Referer':'https//www.proxynova.com/',
            'Sec-Ch-Ua':'"Chromium";v="119", "Not?A_Brand";v="24"',
            'Sec-Ch-Ua-Mobile':'?0',
            'Sec-Ch-Ua-Platform':'"Linux"',
            'Sec-Fetch-Dest':'document',
            'Sec-Fetch-Mode':'navigate',
            'Sec-Fetch-Site':'same-origin',
            'Sec-Fetch-User':'?1',
            'Upgrade-Insecure-Requests':'1',
            'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',

        }
        url = 'https://www.proxynova.com/'
        data = requests.get(url,headers=header).text
        for i in range(data.count('<li><a href="/proxy-server-list/country')):
            region = data.split('<li><a href="/proxy-server-list/country-')[1].split('/"')[0]
            data_prx = requests.get(f'https://www.proxynova.com/proxy-server-list/country-{region}/',headers=header).text
            extract(data_prx,region)
            data = data.replace('<li><a href="/proxy-server-list/country-','encrypt',1)
    def proxyscrape(self):
        proxy = []
        apihttp = 'https://api.proxyscrape.com/?request=getproxies&proxytype=http&timeout=10000&country=all&ssl=all&anonymity=all'
        apisock4 = 'https://api.proxyscrape.com/?request=getproxies&proxytype=socks4&timeout=10000&country=all'
        apisock5 = 'https://api.proxyscrape.com/?request=getproxies&proxytype=socks5&timeout=10000&country=all'
        proxy += (requests.post(apihttp).text.replace('\r','').split('\n'))[:-1] + (requests.post(apisock4).text.replace('\r','').split('\n'))[:-1] + (requests.post(apisock5).text.replace('\r','').split('\n'))[:-1]
        open('proxy.txt','a').write('\n'.join(proxy))
        print(f'''[{self.col['red']}+{self.col['white']}] proxyscrape {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
        return open('proxy.txt','a').write('\n')
    def geonode(self):
        thread = []
        def extract(url,header):
            try:
                proxy = []
                data = requests.get(url,headers=header).json()['data']
                for i in range(len(data)):
                    proxy.append(data[i]['ip']+':'+data[i]['port'])
                open('proxy.txt','a').write('\n'.join(proxy))
                print(f'''[{self.col['red']}+{self.col['white']}] geonode {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
                self.status(proxy)
                return open('proxy.txt','a').write('\n')
            except:
                print(self.col['red']+'get blocked from geonode'+self.col['white'],end = '\r')
                return
        header = {
            'Referer':'https//geonode.com/',
            'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        }
        url = 'https://proxylist.geonode.com/api/proxy-list?limit=100&page=1&sort_by=lastChecked&sort_type=desc'
        total = requests.get(url,headers=header).json()
        if 'message' in total:
            print(self.col['red']+'get blocked from geonode'+self.col['white'])
            return
        elif 'total' in total:
            total = total['total']
        if int(total/100) != total/100:
            total = int(total/100)+1
        else:
            total = int(total/100)
        for i in range(total):
            url = f'https://proxylist.geonode.com/api/proxy-list?limit=100&page={i+1}&sort_by=lastChecked&sort_type=desc'
            extract(url,header)
    def spys(self):
        def extract(data,region):
            try:
                jscode = data.split('</table><script type="text/javascript">')[1].split('\n')[0]
                ip = []
                for i in range(data.count('"><td colspan=1><font class=spy14>')):
                    ip.append(data.split('"><td colspan=1><font class=spy14>')[1].split('<')[0])
                    data = data.replace('"><td colspan=1><font class=spy14>','encrypt',1)
                    port = data[data.index(ip[i]):].split('document.write(')[1].split('</script>')[0]
                    jscode += f'\nconsole.log({port}'
                open('math1.js','w').write(jscode.replace('</script>',''))
                port = str(muterun_js('math1.js').stdout).replace("'",'').replace('b','').replace('</font>','').replace('<font class=spy2>','').split('\\n')
                port.pop(-1)
                for i in range(len(port)):
                    ip[i] += port[i]
                print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] spys {self.col['red']}•{self.col['white']} {len(ip)} proxies added {self.col['red']}•{self.col['white']} {region} {self.col['red']}•{self.col['white']} 100%''',end = '\n')
                open('proxy.txt','a').write('\n'.join(ip))
                open('proxy.txt','a').write('\n')
                self.status(ip)
            except Exception as e:
                print(self.col['red']+'get banned a while from spys',end = '\r')
                time.sleep(0.05)
        header = {
            'Content-Type':'application/x-www-form-urlencoded',
            'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        }
        def data1():
            formdata = 'xpp=5'
            url = 'https://spys.one/proxy-asn/'
            data = requests.post(url,data=formdata,headers=header).text
            for i in range(data.count("'><font class=spy6>")):
                region = data.split("'><font class=spy6>")[1].split(' <')[0]
                data = data.replace("'><font class=spy6>",'encrypt',1)
                url = ('https://spys.one/proxy-city/'+region).replace(' ','-')
                formdata ='xx0=8c2f3d8b99fa294b2084d1b087250a8c&xpp=5&xf1=0&xf2=0&xf5=0'
                datarq = requests.post(url,data=formdata,headers=header).text
                extract(datarq,region)
        def data2():
            formdata = 'xpp=5'
            url = 'https://spys.one/proxy-city/'
            data = requests.post(url,data=formdata,headers=header).text
            for i in range(data.count("'><font class=spy6>")):
                region = data.split("'><font class=spy6>")[1].split(' <')[0]
                data = data.replace("'><font class=spy6>",'encrypt',1)
                url = ('https://spys.one/proxy-city/'+region).replace(' ','-')
                formdata ='xx0=8c2f3d8b99fa294b2084d1b087250a8c&xpp=5&xf1=0&xf2=0&xf5=0'
                datarq = requests.post(url,data=formdata,headers=header).text
                extract(datarq,region)
        thread1 = threading.Thread(target=data1)
        thread2 = threading.Thread(target=data2)
        thread1.start()
        thread2.start()
        thread1.join()
        thread2.join()
    def openproxy(self):
        def thread1():
            proxy = []
            url = 'https://openproxy.space/list/http'
            data = requests.get(url).text
            for i in range(data.count(',items:[')):
                data_scrape = data.split(',items:[')[1].split(']')[0].replace('"','').split(',')
                data = data.replace(',items:[','encrypt',1)
                for i in data_scrape:
                    proxy.append(i)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] openproxy {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        def thread2():
            proxy = []
            url = 'https://openproxy.space/list/socks4'
            data = requests.get(url).text
            for i in range(data.count(',items:[')):
                data_scrape = data.split(',items:[')[1].split(']')[0].replace('"','').split(',')
                data = data.replace(',items:[','encrypt',1)
                for i in data_scrape:
                    proxy.append(i)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] openproxy {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        def thread3():
            proxy = []
            url = 'https://openproxy.space/list/socks5'
            data = requests.get(url).text
            for i in range(data.count(',items:[')):
                data_scrape = data.split(',items:[')[1].split(']')[0].replace('"','').split(',')
                data = data.replace(',items:[','encrypt',1)
                for i in data_scrape:
                    proxy.append(i)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] openproxy {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        data1 = threading.Thread(target=thread1)
        data2 = threading.Thread(target=thread2)
        data3 = threading.Thread(target=thread3)
        data1.start()
        data2.start()
        data3.start()
    def proxysitelist(self):
        url = 'https://proxysitelist.net/'
        data = requests.get(url).text
        proxy = []
        for i in range(data.count('</tr><tr><td>')):
            ip = data.split('</tr><tr><td>')[1].split('<')[0]
            port = data[data.index(ip):].split('</td><td>')[1].split('<')[0]
            proxy.append(ip+':'+port)
            data = data.replace('</tr><tr><td>','encrypt',1)
        open('proxy.txt','a').write('\n'.join(proxy))
        open('proxy.txt','a').write('\n')
        print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] proxysitelist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
    def plsource(self):
        url = 'https://proxylist.contact1777.workers.dev/'
        data = requests.get(url).text
        proxy = []
        for i in range(data.count('<tr><td>')):
            ip = data.split('<tr><td>')[1].split('<')[0]
            port = data[data.index(ip):].split('</td><td>')[1].split('<')[0]
            proxy.append(ip+':'+port)
            data = data.replace('<tr><td>','encrypt',1)
        open('proxy.txt','a').write('\n'.join(proxy))
        open('proxy.txt','a').write('\n')
        print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] plsource {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
    def rotatingproxies(self):
        url = 'https://rotatingproxies.com/free/get-proxies.php?proxy_anonymity=all&proxy_protocol=all&proxy_location=Please%20Select'
        formdata = 'proxy_anonymity=all&proxy_protocol=all&proxy_location=Please%20Select'
        header = {
            'Accept':'application/json, text/javascript, */*; q=0.01',
            'Referer':'https//rotatingproxies.com/free/',
            'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'X-Requested-With':'XMLHttpRequest',
        }
        data = requests.get(url,data=formdata,headers=header).json()
        if 'error' in data:
            print(self.col['red']+'get blocked api from rotatingproxies') 
            return
        data.pop('_links')
        data.pop('cache')
        data.pop('stats')
        proxy = []
        for i in data:
            proxy.append(data[i]['ip']+':'+str(data[i]['port']))
        open('proxy.txt','a').write('\n'.join(proxy))
        open('proxy.txt','a').write('\n')
        print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] rotatingproxies {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
    def proxylistorg(self):
        data = "Proxy('"
        proxy = []
        page = 0
        while "Proxy('" in data:
            page += 1
            url = f'https://proxy-list.org/english/search.php?p={page}'
            data = requests.get(url).text
            cnt = data.count("Proxy('")
            for i in range(cnt):
                proxy.append(self.decode(data.split("Proxy('")[1].split("'")[0]))
                data = data.replace("Proxy('",'encrypt',1)
            if cnt != 0:
                print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] proxylistorg {self.col['red']}•{self.col['white']} {i+1} proxies found {self.col['red']}•{self.col['white']} page {page} {self.col['red']}•{self.col['white']} 100%''',end = '\n')
                data = "Proxy('"
        open('proxy.txt','a').write('\n'.join(proxy))
        open('proxy.txt','a').write('\n')
        print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] proxylistorg {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
    def thespeedx(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] thespeedx {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args= ('https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks5.txt',))
        th2 = threading.Thread(target=extract,args= ('https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks4.txt',))
        th3 = threading.Thread(target=extract,args= ('https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt',))
        th1.start()
        th2.start()
        th3.start()
        th1.join()
        th2.join()
        th3.join()
    def hidemyip(self):
        try:
            url = 'https://www.hide-my-ip.com/proxylist.shtml'
            header = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',}
            data = requests.get(url,headers=header).text
            json = data.split('var json = \n')[1].split(']')[0]
            proxy = []
            while '{"i":"' in data:
                proxy.append(data.split('{"i":"')[1].split('"')[0]+':'+data.split('"p":"')[1].split('"')[0])
                data = data.replace('"p":"','',1).replace('{"i":"','',1)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] hidemyip {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        except:
            print(self.col['red']+'da xay ra loi, dang khac phuc'+self.col['white'],end = '\r')
    def freeproxyupdate(self):
        def extract(region):
            try:
                proxy = []
                url = f'https://freeproxyupdate.com/{region}'
                data = requests.get(url).text.replace('<tr>','encrypt',1)
                for i in range(data.count('<tr>')):
                    ip = data[data.index('<tr>'):].split('<td>')[1].split('<')[0]
                    port = data[data.index(ip):].split('<td>')[1].split('<')[0]
                    data = data.replace('<tr>','encrypt',1)
                    proxy.append(f'{ip}:{port}')
                open('proxy.txt','a').write('\n'.join(proxy))
                open('proxy.txt','a').write('\n')
                print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] freeproxyupdate {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} {region} {self.col['red']}•{self.col['white']} 100%''',end = '\n')
                self.status(proxy)
            except:
                print(self.col['red']+'get blocked from freeproxyupdate'+self.col['white'],end = '\r')
        url = 'https://freeproxyupdate.com/'
        data = requests.get(url).text
        thread = []
        for i in range(data.count('<li class="')):
            region = data[data.index('<li class="'):].split('href="/')[1].split('"')[0]
            data = data.replace('<li class="','encrypt',1)
            extract(region)
    def zloi_user(self):
        def extract(url):
            data = requests.get(url).text.split('\n')
            proxy = []
            for i in data:
                info = i.split(':')
                if info == ['']:
                    continue
                proxy.append(info[0]+':'+info[1])
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] zloi-user {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/zloi-user/hideip.me/main/http.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/zloi-user/hideip.me/main/https.txt',))
        th3 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/zloi-user/hideip.me/main/socks4.txt',))
        th4 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/zloi-user/hideip.me/main/socks5.txt',))
        th5 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/zloi-user/hideip.me/main/connect.txt',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
        th5.start()
    def proxyspacepro(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] proxyspacepro {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://proxyspace.pro/socks4.txt',))
        th2 = threading.Thread(target=extract,args=('https://proxyspace.pro/socks5.txt',))
        th3 = threading.Thread(target=extract,args=('https://proxyspace.pro/http.txt',))
        th4 = threading.Thread(target=extract,args=('https://proxyspace.pro/https.txt',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
    def openproxylist(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] openproxylist {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://api.openproxylist.xyz/socks4.txt',))
        th2 = threading.Thread(target=extract,args=('https://api.openproxylist.xyz/socks5.txt',))
        th3 = threading.Thread(target=extract,args=('https://api.openproxylist.xyz/http.txt',))
        th1.start()
        th2.start()
        th3.start()
    def proxylistdownload(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] proxylistdownload {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://proxy-list.download/api/v1/get?type=socks4',))
        th2 = threading.Thread(target=extract,args=('https://proxy-list.download/api/v1/get?type=socks5',))
        th3 = threading.Thread(target=extract,args=('https://proxy-list.download/api/v1/get?type=http',))
        th4 = threading.Thread(target=extract,args=('https://proxy-list.download/api/v1/get?type=https',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
    def allilapro(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] allilapro {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/ALIILAPRO/Proxy/main/http.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/ALIILAPRO/Proxy/main/socks4.txt',))
        th3 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/ALIILAPRO/Proxy/main/socks5.txt',))
        th4 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
    def murongpig(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] murongpig {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/MuRongPIG/Proxy-Master/main/socks4.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/MuRongPIG/Proxy-Master/main/socks5.txt',))
        th3 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/MuRongPIG/Proxy-Master/main/http.txt',))
        th1.start()
        th2.start()
        th3.start()
    def kangproxy(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] kangproxy {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/http/http.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/https/https.txt',))
        th3 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/socks4/socks4.txt',))
        th4 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/socks5/socks5.txt',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
    def prxchk(self):
        def extract(url):
            proxy = requests.get(url).text.replace('//','').replace('socks4:','').replace('socks5','').replace('http','').split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] prxchk {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        extract('https://raw.githubusercontent.com/prxchk/proxy-list/main/all.txt')
    def roosterkid(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] roosterkid {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/roosterkid/openproxylist/main/SOCKS4_RAW.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/roosterkid/openproxylist/main/SOCKS5_RAW.txt',))
        th1.start()
        th2.start()
    def zaeem20(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] zaeem20 {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        th1 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/socks4.txt',))
        th2 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/socks5.txt',))
        th3 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/http.txt',))
        th4 = threading.Thread(target=extract,args=('https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/https.txt',))
        th1.start()
        th2.start()
        th3.start()
        th4.start()
    def zevtyardt(self):
        def extract(url):
            proxy = requests.get(url).text.split('\n')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] zevtyardt {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        extract('https://raw.githubusercontent.com/zevtyardt/proxy-list/main/all.txt')
    def hidemysecret(self):
        header = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',}
        url = 'https://hidemy.io/en/proxy-list/countries/'
        data = requests.get(url,headers=header).text
        def extractdata(region,header):
            extract = []
            data = requests.get(f'https://hidemy.io/en/proxy-list/countries/{region}',headers=header).text
            data = data.replace('<tr><td>','encrypthngl',1)
            while '<tr><td>' in data:
                proxy = data.split('<tr><td>')[1].split('<')[0]
                if '.' in proxy:
                    extract.append(proxy)
                    port = data[data.index(extract[-1]):].split('</td><td>')[1].split('<')[0]
                    extract[-1] += ':'+port
                    data = data.replace('<tr><td>','encrypthngl',1).replace(f'</td><td>{port}<','encrypthngl',1)
                else:
                    break
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] hidemysecret {self.col['red']}•{self.col['white']} {len(extract)} proxies added {self.col['red']}•{self.col['white']} {region[:-1]} {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(extract)
            return extract
        dh = 'class="flag-icon flag-icon-'
        for i in range(data.count(dh)):
            region = data[data.index(dh):].split('countries/')[1].split('"')[0]
            data = data.replace(dh,'',1)
            extractdata(region,header)
    def freeproxyworld(self):
        page,data = 0,None
        def extract(url):
            data = requests.get(url).text
            proxy = []
            for i in range(data.count('<td class="show-ip-div">')):
                ip = data.split('<td class="show-ip-div">\n')[1].split('\n')[0]
                port = data[data.index(ip):].split('<a href="/?port=')[1].split('"')[0]
                proxy.append(ip+':'+port)
                data = data.replace('<td class="show-ip-div">','',1)
            if proxy == []:
                return False
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] freeproxyworld {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        while data != False:
            page += 1
            url = 'https://www.freeproxy.world/?type=&anonymity=&country=&speed=&port=&page='+str(page)
            data = extract(url)
    def abcproxy(self):
        api = 'https://check.abcproxy.com/free_ip/get_list'
        header = {'Content-Type':'application/x-www-form-urlencoded; charset=UTF-8'}
        thread,response,page = [],True,0
        def extract(api,formdata):
            data = requests.post(api,data=formdata,headers=header).json()['ret_data']['data']
            proxy = []
            for i in data:
                proxy.append(i['ip']+':'+i['port'])
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] abcproxy {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
            return proxy
        while response:
            page += 1
            formdata = f'proxy_type=&anon=&country_name=&port=&delay=&page={page}'
            try:
                data = extract(api,formdata)
                if len(data)==0:
                	break
                thread.append(threading.Thread(target=extract,args=(api,formdata,)))
                thread[page-1].start()
            except:
                break
    def iproyal(self):
        def extract(data):
            proxy = []
            for i in range(data.count('"><div class="flex items-center astro-lmapxigl">')):
                ip = data.split('"><div class="flex items-center astro-lmapxigl">')[1].split('<')[0]
                port = data[data.index(ip):].split('">')[1].split('<')[0]
                proxy.append(f'{ip}:{port}')
                data = data.replace('"><div class="flex items-center astro-lmapxigl">','encrypt',1)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] iproyal {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        data = requests.get('https://iproyal.com/free-proxy-list/?page=1&entries=100').text
        id = data.split('link[data-v-')[1].split(']')[0]
        for i in range(data.count(f'class="px-8 sm:px-16 pagination-link" data-v-{id}>')):
            value = data.split(f'class="px-8 sm:px-16 pagination-link" data-v-{id}>')[1].split('<')[0]
            data = data.replace(f'class="px-8 sm:px-16 pagination-link" data-v-{id}>'+value,'encrypt',1)
        for i in range(int(value)):
            url = f'https://iproyal.com/free-proxy-list/?page={i+1}&entries=100'
            data = requests.get(url).text
            extract(data)
    def vpnside(self):
        proxy = []
        url = 'https://www.vpnside.com/proxy/list/'
        data = requests.get(url).text
        cnt = data.count('<tr id="table_1_row_')
        for i in range(cnt-1):
            ip = data[data.index(f'<tr id="table_1_row_{i+1}'):].split('<td style>')[1].split('<')[0]
            port = data[data.index(ip):].split('<td style>')[1].split('<')[0]
            proxy.append(f'{ip}:{port}')
        open('proxy.txt','a').write('\n'.join(proxy))
        open('proxy.txt','a').write('\n')
        print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] vpnside {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
        self.status(proxy)
    def iplocation(self):
        def extract(data):
            proxy = []
            for i in range(data.count("<td><a href='/?query=")):
                ip = data.split("<td><a href='/?query=")[1].split("'")[0]
                port = data[data.index(ip):].split('<td>')[1].split('<')[0]
                proxy.append(f'{ip}:{port}')
                data = data.replace("<td><a href='/?query=",'',1)
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] iplocation {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        url = 'https://www.iplocation.net/proxy-list/index'
        data = requests.get(url).text
        for i in range(data.count('index/')):
            lastpage = data.split('index/')[1].split('"')[0]
            data = data.replace('index/','',1)
        for i in range(int(lastpage)):
            url = f'https://www.iplocation.net/proxy-list/index/{i+1}'
            extract(requests.get(url).text)
    def advancedname(self):
        def extract(data):
            proxy = []
            for i in range(data.count('data-ip="')):
                ip = self.decode(data.split('data-ip="')[1].split('"')[0])
                port = self.decode(data.split('data-port="')[1].split('"')[0])
                data = data.replace('data-port="','',1).replace('data-ip="','',1)
                proxy.append(f'{ip}:{port}')
            open('proxy.txt','a').write('\n'.join(proxy))
            open('proxy.txt','a').write('\n')
            print(f'''{self.col['white']}[{self.col['red']}+{self.col['white']}] advancedname {self.col['red']}•{self.col['white']} {len(proxy)} proxies added {self.col['red']}•{self.col['white']} 100%''',end = '\n')
            self.status(proxy)
        url = 'https://advanced.name/freeproxy'
        data = requests.get(url).text
        page = 0
        for i in range(data.count('page=')):
            pagefound = data.split('page=')[1].split('"')[0]
            data = data.replace('page=','',1)
            if int(pagefound) > page:
                page = int(pagefound)
        for i in range(page):
            url = f'https://advanced.name/freeproxy/?page={i+1}'
            data = requests.get(url,).text
            extract(data)
def running():
    response = mining()
    #test = response.abcproxy()
    #---------------------thread
    thdg1 = threading.Thread(target=response.proxynova)
    thdg2 = threading.Thread(target=response.hidemy_io)
    thdg3 = threading.Thread(target=response.vpnoverview)
    thdg4 = threading.Thread(target=response.free_proxy_list)
    thdg5 = threading.Thread(target=response.proxyscrape)
    thdg6 = threading.Thread(target=response.geonode)
    thdg7 = threading.Thread(target=response.spys)
    thdg8 = threading.Thread(target=response.openproxy)
    thdg9 = threading.Thread(target=response.proxysitelist)
    thdg10 = threading.Thread(target=response.plsource)
    thdg11 = threading.Thread(target=response.rotatingproxies)
    thdg12 = threading.Thread(target=response.proxylistorg)
    thdg13 = threading.Thread(target=response.thespeedx)
    thdg14 = threading.Thread(target=response.hidemyip)
    thdg15 = threading.Thread(target=response.freeproxyupdate)
    thdg16 = threading.Thread(target=response.zloi_user)
    thdg17 = threading.Thread(target=response.proxyspacepro)
    thdg18 = threading.Thread(target=response.openproxylist)
    thdg19 = threading.Thread(target=response.proxylistdownload)
    thdg20 = threading.Thread(target=response.allilapro)
    thdg21 = threading.Thread(target=response.murongpig)
    thdg22 = threading.Thread(target=response.kangproxy)
    thdg23 = threading.Thread(target=response.prxchk)
    thdg24 = threading.Thread(target=response.roosterkid)
    thdg25 = threading.Thread(target=response.zaeem20)
    thdg26 = threading.Thread(target=response.zevtyardt)
    thdg27 = threading.Thread(target=response.hidemysecret)
    thdg28 = threading.Thread(target=response.freeproxyworld)
    thdg29 = threading.Thread(target=response.abcproxy)
    thdg30 = threading.Thread(target=response.iproyal)
    thdg31 = threading.Thread(target=response.vpnside)
    thdg32 = threading.Thread(target=response.iplocation)
    thdg33 = threading.Thread(target=response.advancedname)
    #----------------------run
    thdg1.start()
    thdg2.start()
    thdg3.start()
    thdg4.start()
    thdg5.start()
    thdg6.start()
    thdg7.start()
    thdg8.start()
    thdg9.start()
    thdg10.start()
    thdg11.start()
    thdg12.start()
    thdg13.start()
    thdg14.start()
    thdg15.start()
    thdg16.start()
    thdg17.start()
    thdg18.start()
    thdg19.start()
    thdg20.start()
    thdg21.start()
    thdg22.start()
    thdg23.start()
    thdg24.start()
    thdg25.start()
    thdg26.start()
    thdg27.start()
    thdg28.start()
    thdg29.start()
    thdg30.start()
    thdg31.start()
    thdg32.start()
    thdg33.start()
    #---------------------stack
    thdg1.join()
    thdg2.join()
    thdg3.join()
    thdg4.join()
    thdg5.join()
    thdg6.join()
    thdg7.join()
    thdg8.join()
    thdg9.join()
    thdg11.join()
    thdg12.join()
    thdg13.join()
    thdg14.join()
    thdg15.join()
    thdg16.join()
    thdg17.join()
    thdg18.join()
    thdg19.join()
    thdg20.join()
    thdg21.join()
    thdg22.join()
    thdg23.join()
    thdg24.join()
    thdg25.join()
    thdg26.join()
    thdg27.join()
    thdg28.join()
    thdg29.join()
    thdg30.join()
    thdg31.join()
    thdg32.join()
    thdg33.join()
    response.total()
if __name__=='__main__':   
    running()
